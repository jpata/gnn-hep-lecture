{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ec966a1",
   "metadata": {},
   "source": [
    "## Colab setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5696cc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas numpy awkward0 uproot3_methods matplotlib\n",
    "# !pip3 install torch torchvision torchaudio\n",
    "\n",
    "# import os\n",
    "# import torch\n",
    "# os.environ['TORCH'] = torch.__version__\n",
    "# print(torch.__version__)\n",
    "\n",
    "# !pip install -q torch-geometric -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21897ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as nps\n",
    "import awkward0 as awkward\n",
    "import uproot3_methods as uproot_methods\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc43a69",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dff68e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc -O test.h5 https://zenodo.org/record/2603256/files/test.h5?download=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a9158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf(\"test.h5\", key=\"table\", start=0, stop=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63988a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4280f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on https://github.com/hqucms/ParticleNet/blob/master/tf-keras/convert_dataset.ipynb\n",
    "def _col_list(prefix, max_particles=200):\n",
    "    return ['%s_%d'%(prefix,i) for i in range(max_particles)]\n",
    "\n",
    "def get_constituents(df):\n",
    "    _px = df[_col_list('PX')].values\n",
    "    _py = df[_col_list('PY')].values\n",
    "    _pz = df[_col_list('PZ')].values\n",
    "    _e = df[_col_list('E')].values\n",
    "\n",
    "    mask = _e>0\n",
    "    n_particles = np.sum(mask, axis=1)\n",
    "\n",
    "    px = awkward.JaggedArray.fromcounts(n_particles, _px[mask])\n",
    "    py = awkward.JaggedArray.fromcounts(n_particles, _py[mask])\n",
    "    pz = awkward.JaggedArray.fromcounts(n_particles, _pz[mask])\n",
    "    energy = awkward.JaggedArray.fromcounts(n_particles, _e[mask])\n",
    "\n",
    "    p4 = uproot_methods.TLorentzVectorArray.from_cartesian(px, py, pz, energy)\n",
    "    jet_p4 = p4.sum()\n",
    "\n",
    "    eta = jet_p4.eta - p4.eta\n",
    "    phi = jet_p4.delta_phi(p4)\n",
    "    pt = p4.pt / jet_p4.pt\n",
    "    label = df['is_signal_new'].values\n",
    "    \n",
    "    return pt, eta, phi, label\n",
    "\n",
    "pt, eta, phi, label = get_constituents(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7262621",
   "metadata": {},
   "source": [
    "## Self-attention based encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27c018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, num_node_features=3, embed_dim=128):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.embed1 = torch.nn.Linear(num_node_features, 128)\n",
    "        self.embed2 = torch.nn.Linear(128, embed_dim)\n",
    "        \n",
    "        self.norm1 = torch.nn.LayerNorm(embed_dim)\n",
    "        self.attn1 = nn.MultiheadAttention(\n",
    "            embed_dim,\n",
    "            8,\n",
    "            dropout=0.0,\n",
    "            add_bias_kv=False,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm2 = torch.nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.out1 = torch.nn.Linear(embed_dim, 128)\n",
    "        self.out2 = torch.nn.Linear(128, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.selu(self.embed1(x))\n",
    "        x = self.embed2(x)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        x_mask = (x[:, :, 0]==0) & (x[:, :, 1]==0)\n",
    "        x_attn, attention_matrix = self.attn1(x,x,x, key_padding_mask=x_mask)\n",
    "        \n",
    "        x_mask_f = (~x_mask).to(dtype=torch.float32).unsqueeze(axis=-1)\n",
    "        x = x+x_attn*x_mask_f\n",
    "        x = self.norm2(x)*x_mask_f\n",
    "        \n",
    "        x_sum = torch.sum(x, axis=-2)\n",
    "        \n",
    "        x_sum = torch.selu(self.out1(x_sum))\n",
    "        out = torch.sigmoid(self.out2(x_sum))\n",
    "        \n",
    "        return x, attention_matrix, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaab5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084ea283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8654c2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, x_feats, y_vals):\n",
    "        'Initialization'\n",
    "        self.x_feats = x_feats\n",
    "        self.y_vals = y_vals\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.x_feats)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return x_feats[index], y_vals[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbebe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feats = [torch.tensor(np.stack([pt[i],eta[i],phi[i]], axis=-1)) for i in range(len(df))]\n",
    "y_vals = torch.stack([torch.tensor(label[i], dtype=torch.float32) for i in range(len(df))])\n",
    "\n",
    "ds = Dataset(x_feats, y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb7603",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ba3652",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_out, attention_matrix, out = n(x.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6236a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x.numpy())\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c77f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_out[0].detach().cpu().numpy())\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e00254",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(attention_matrix.detach().cpu().numpy()[0])\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(x_out[0])));\n",
    "plt.yticks(range(len(x_out[0])));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dfb216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(inputs):\n",
    "    return pad_sequence([i[0] for i in inputs], batch_first=True), torch.stack([i[1] for i in inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a72a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = torch.utils.data.DataLoader(ds, batch_size=128, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910bd0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Net(embed_dim=256).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)\n",
    "\n",
    "model.train()\n",
    "losses_train = []\n",
    "\n",
    "for epoch in range(5):\n",
    "    \n",
    "    loss_train_epoch = []\n",
    "    \n",
    "    for X, y in tqdm.tqdm(training_generator):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(X)[2]\n",
    "        loss = torch.nn.functional.binary_cross_entropy(out[:, 0], y)\n",
    "\n",
    "        loss.backward()\n",
    "        loss_train_epoch.append(loss.item())\n",
    "        optimizer.step()\n",
    "\n",
    "    loss_train_epoch = np.mean(loss_train_epoch)\n",
    "    losses_train.append(loss_train_epoch)\n",
    "    print(epoch, loss_train_epoch)\n",
    "    \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52562e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_attn, attn_matrix, out = model(ds[0][0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1a6261",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(attn_matrix.detach().cpu().numpy()[0])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.concat([model(d[0])[2][:, 0] for d in training_generator])\n",
    "y_true = torch.concat([d[1] for d in training_generator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f945a64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape, y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c59bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.linspace(0,1,21)\n",
    "plt.hist(y_pred[y_true==1].detach().numpy(), bins=b, histtype=\"step\", lw=2);\n",
    "plt.hist(y_pred[y_true==0].detach().numpy(), bins=b, histtype=\"step\", lw=2);\n",
    "#plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e8fd77",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e9acde",
   "metadata": {},
   "source": [
    "### 1. Add another stacked attention layer, check the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bd0d9d",
   "metadata": {},
   "source": [
    "### 2. Query the second attention layer with a learnable vector, instead of the encoded elements. Check the performance of the model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d95a4cc",
   "metadata": {},
   "source": [
    "### 3. Change the model to output a per-particle classification score (e.g. PU rejection in a jet).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
