{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21897ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as nps\n",
    "import awkward0 as awkward\n",
    "import uproot3_methods as uproot_methods\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a9158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf(\"test.h5\", key=\"table\", start=0, stop=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4280f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on https://github.com/hqucms/ParticleNet/blob/master/tf-keras/convert_dataset.ipynb\n",
    "def _col_list(prefix, max_particles=200):\n",
    "    return ['%s_%d'%(prefix,i) for i in range(max_particles)]\n",
    "\n",
    "def get_constituents(df):\n",
    "    _px = df[_col_list('PX')].values\n",
    "    _py = df[_col_list('PY')].values\n",
    "    _pz = df[_col_list('PZ')].values\n",
    "    _e = df[_col_list('E')].values\n",
    "\n",
    "    mask = _e>0\n",
    "    n_particles = np.sum(mask, axis=1)\n",
    "\n",
    "    px = awkward.JaggedArray.fromcounts(n_particles, _px[mask])\n",
    "    py = awkward.JaggedArray.fromcounts(n_particles, _py[mask])\n",
    "    pz = awkward.JaggedArray.fromcounts(n_particles, _pz[mask])\n",
    "    energy = awkward.JaggedArray.fromcounts(n_particles, _e[mask])\n",
    "\n",
    "    p4 = uproot_methods.TLorentzVectorArray.from_cartesian(px, py, pz, energy)\n",
    "    jet_p4 = p4.sum()\n",
    "\n",
    "    eta = jet_p4.eta - p4.eta\n",
    "    phi = jet_p4.delta_phi(p4)\n",
    "    pt = p4.pt / jet_p4.pt\n",
    "    label = df['is_signal_new'].values\n",
    "    \n",
    "    return pt, eta, phi, label\n",
    "\n",
    "pt, eta, phi, label = get_constituents(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27c018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, num_node_features=3, embed_dim=16):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.embed = torch.nn.Linear(num_node_features, embed_dim)\n",
    "        \n",
    "        self.norm1 = torch.nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim,\n",
    "            8,\n",
    "            dropout=0.0,\n",
    "            add_bias_kv=False,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm2 = torch.nn.LayerNorm(embed_dim)\n",
    "        self.out1 = torch.nn.Linear(embed_dim, 128)\n",
    "        self.out2 = torch.nn.Linear(128, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        x_mask = (x_feats[:, :, 0]==0) & (x_feats[:, :, 1]==0)\n",
    "        x_attn, attention_matrix = self.attn(x,x,x, key_padding_mask=x_mask)\n",
    "        \n",
    "        x_mask_f = (~x_mask).to(dtype=torch.float32).unsqueeze(axis=-1)\n",
    "        x = x+x_attn*x_mask_f\n",
    "        x = self.norm2(x)*x_mask_f\n",
    "                \n",
    "        x_sum = torch.sum(x, axis=-2)\n",
    "        \n",
    "        x_sum = torch.selu(self.out1(x_sum))\n",
    "        out = torch.sigmoid(self.out2(x_sum))\n",
    "        \n",
    "        return x, attention_matrix, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaab5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084ea283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbebe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feats = [torch.tensor(np.stack([pt[i],eta[i],phi[i]], axis=-1)) for i in range(2000)]\n",
    "y_vals = torch.stack([torch.tensor(label[i], dtype=torch.float32) for i in range(2000)])\n",
    "x_feats = pad_sequence(x_feats, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb7603",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc74ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vals.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ba3652",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, attention_matrix, out = n(x_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6236a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_feat.cpu().numpy()[0], cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c77f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x.detach().cpu().numpy()[0], cmap=\"bwr\", norm=matplotlib.colors.Normalize(vmin=-2,vmax=2))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e00254",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(attention_matrix.detach().cpu().numpy()[2], cmap=\"bwr\", norm=matplotlib.colors.Normalize(vmin=-0.1,vmax=0.1))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910bd0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Net(embed_dim=256).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "model.train()\n",
    "losses_train = []\n",
    "\n",
    "for epoch in range(20):\n",
    "    \n",
    "    loss_train_epoch = []\n",
    "    \n",
    "    x_feats = x_feats.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x_feats)[2]\n",
    "    loss = torch.nn.functional.binary_cross_entropy(out[:, 0], y_vals)\n",
    "\n",
    "    loss.backward()\n",
    "    loss_train_epoch.append(loss.item())\n",
    "    optimizer.step()\n",
    "        \n",
    "    loss_train_epoch = np.mean(loss_train_epoch)\n",
    "    losses_train.append(loss_train_epoch)\n",
    "    print(epoch, loss_train_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52562e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_attn, attn_matrix, out = model(x_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1a6261",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(attn_matrix.detach().cpu().numpy()[8], cmap=\"bwr\", norm=matplotlib.colors.Normalize(vmin=-0.1,vmax=0.1))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c59bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.linspace(0,1,21)\n",
    "plt.hist(out[y_vals==1].detach().numpy(), bins=b, histtype=\"step\", lw=2);\n",
    "plt.hist(out[y_vals==0].detach().numpy(), bins=b, histtype=\"step\", lw=2);\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edab0b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
